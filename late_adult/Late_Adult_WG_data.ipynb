{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ce8c42-3a82-44ea-914d-2fd4e9756138",
   "metadata": {},
   "source": [
    "# How to Get Raw-QCed Cardiovascular Data?\n",
    "\n",
    "To know more about solr requests, go to the workshop [repository](https://github.com/mpi2/impc-data-api-workshop/tree/main) with all materials.\n",
    "\n",
    "### To download the data\n",
    "\n",
    "### Set up helper functions\n",
    "1. `solr_request` — Performs a single Solr request\n",
    "2. `batch_request` — Calls `solr_request` multiple times with `params` to retrieve results in chunk `batch_size` rows at a time.\n",
    "3. `facet_request` — Performs a faceting Solr request\n",
    "4. `entity_iterator` — Generator function fetches results from the Solr server in chunks using pagination.\n",
    "5. `iterator_solr_request` — Fetches results in batches from the Solr API and write them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd1a00e5-1490-484e-b26b-7db277c87987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from urllib.parse import unquote\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Display the whole dataframe <15\n",
    "# pd.set_option('display.max_rows', 15)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Create helper function\n",
    "def solr_request(core, params, silent=False):\n",
    "    \"\"\"Performs a single Solr request.\n",
    "    \n",
    "    Returns:\n",
    "        num_found: How many rows in total did the request match.\n",
    "        df: A Pandas dataframe with a portion of the request matching `start` and `rows` parameters.\n",
    "        silent: Suppress displaying the df and number of results (useful for batch requests).\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.ebi.ac.uk/mi/impc/solr/\"\n",
    "    solr_url = base_url + core + \"/select\"\n",
    "\n",
    "    response = requests.get(solr_url, params=params)\n",
    "    if not silent:\n",
    "        print(f\"\\nYour request:\\n{response.request.url}\\n\")\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        num_found = data[\"response\"][\"numFound\"]\n",
    "        if not silent:\n",
    "            print(f'Number of found documents: {num_found}\\n')\n",
    "        # Extract and add search results to the list\n",
    "        search_results = []\n",
    "        for doc in data[\"response\"][\"docs\"]:\n",
    "            search_results.append(doc)\n",
    "    \n",
    "        # Convert the list of dictionaries into a DataFrame and print the DataFrame\n",
    "        df = pd.DataFrame(search_results)\n",
    "        if not silent:\n",
    "            display(df)\n",
    "        return num_found, df\n",
    "    \n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)\n",
    "\n",
    "def batch_request(core, params, batch_size):\n",
    "    \"\"\"Calls `solr_request` multiple times with `params` to retrieve results in chunk `batch_size` rows at a time.\"\"\"\n",
    "    if \"rows\" in \"params\":\n",
    "        print(\"WARN: You have specified the `params` -> `rows` value. It will be ignored, because the data is retrieved `batch_size` rows at a time.\")\n",
    "    # Determine the total number of rows. Note that we do not request any data (rows = 0).\n",
    "    num_results, _ = solr_request(core=core, params={**params, \"start\": 0, \"rows\": 0}, silent=True)\n",
    "    # Initialise everything for data retrieval.\n",
    "    start = 0\n",
    "    chunks = []\n",
    "    # Request chunks until we have complete data.\n",
    "    with tqdm(total=num_results) as pbar:  # Initialize tqdm progress bar.\n",
    "        while start < num_results:\n",
    "            # Update progress bar with the number of rows requested.\n",
    "            pbar.update(batch_size) \n",
    "            # Request chunk. We don't need num_results anymore because it does not change.\n",
    "            _, df_chunk = solr_request(core=core, params={**params, \"start\": start, \"rows\": batch_size}, silent=True)\n",
    "            # Record chunk.\n",
    "            chunks.append(df_chunk)\n",
    "            # Increment start.\n",
    "            start += batch_size\n",
    "    # Prepare final dataframe.\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "def facet_request(core, params, silent=False):\n",
    "    \"\"\"Performs a faceting Solr request.\n",
    "    \n",
    "    Returns:\n",
    "        num_found: How many rows in total did the request match.\n",
    "        df: A Pandas dataframe with a portion of the request matching `start` and `rows` parameters.\n",
    "        silent: Suppress displaying the df and number of results (useful for batch requests).\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.ebi.ac.uk/mi/impc/solr/\"\n",
    "    solr_url = base_url + core + \"/select\"\n",
    "\n",
    "    response = requests.get(solr_url, params=params)\n",
    "    if not silent:\n",
    "        print(f\"\\nYour request:\\n{unquote(response.request.url)}\\n\")\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        num_found = data[\"response\"][\"numFound\"]\n",
    "        if not silent:\n",
    "            print(f'Number of found documents: {num_found}\\n')\n",
    "        # Extract and add faceting query results to the list\n",
    "        facet_counts = data[\"facet_counts\"][\"facet_fields\"][params[\"facet.field\"]]\n",
    "        # Initialize an empty dictionary\n",
    "        faceting_dict = {}\n",
    "        # Iterate over the list, taking pairs of elements\n",
    "        for i in range(0, len(facet_counts), 2):\n",
    "            # Assign label as key and count as value\n",
    "            label = facet_counts[i]\n",
    "            count = facet_counts[i + 1]\n",
    "            faceting_dict[label] = [count]\n",
    "        \n",
    "        # Print the resulting dictionary\n",
    "        # Convert the list of dictionaries into a DataFrame and print the DataFrame\n",
    "        df = pd.DataFrame(faceting_dict)\n",
    "        df = pd.DataFrame.from_dict(faceting_dict, orient='index', columns=['counts']).reset_index()\n",
    "\n",
    "        # Rename the columns\n",
    "        df.columns = [params[\"facet.field\"], 'count_per_category']\n",
    "        if not silent:\n",
    "            display(df)\n",
    "        return num_found, df\n",
    "    \n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)\n",
    "\n",
    "# Helper function to fetch results. This function is used by the 'iterator_solr_request' function.\n",
    "def entity_iterator(base_url, params):\n",
    "    \"\"\"Generator function to fetch results from the SOLR server in chunks using pagination\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the Solr server to fetch documents from.\n",
    "        params (dict): A dictionary of parameters to include in the GET request. Must include\n",
    "                       'start' and 'rows' keys, which represent the index of the first document\n",
    "                       to fetch and the number of documents to fetch per request, respectively.\n",
    "\n",
    "    Yields:\n",
    "        dict: The next document in the response from the Solr server.\n",
    "    \"\"\"\n",
    "    # Initialise variable to check the first request\n",
    "    first_request = True\n",
    "\n",
    "    # Call the API in chunks and yield the documents in each chunk\n",
    "    while True:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "        docs = data[\"response\"][\"docs\"]\n",
    "\n",
    "        # Print the first request only\n",
    "        if first_request:\n",
    "            print(f'Your first request: {response.url}')\n",
    "            first_request = False\n",
    "\n",
    "        # Yield the documents in the current chunk\n",
    "        for doc in docs:\n",
    "            yield doc\n",
    "\n",
    "        # Check if there are more results to fetch\n",
    "        start = params[\"start\"] + params[\"rows\"]\n",
    "        num_found = data[\"response\"][\"numFound\"]\n",
    "        if start >= num_found:\n",
    "            break\n",
    "\n",
    "        # Update the start parameter for the next request\n",
    "        params[\"start\"] = start\n",
    "\n",
    "    # Print last request and total number of documents retrieved\n",
    "    print(f'Your last request: {response.url}')\n",
    "    print(f'Number of found documents: {data[\"response\"][\"numFound\"]}\\n')\n",
    "\n",
    "# Function to iterate over field list and write results to a file.\n",
    "def iterator_solr_request(core, params, filename='iteration_solr_request', format='json'):\n",
    "    \"\"\"Function to fetch results in batches from the Solr API and write them to a file\n",
    "        Defaults to fetching 5000 rows at a time.\n",
    "\n",
    "    Args:\n",
    "        core (str): The name of the Solr core to fetch results from.\n",
    "        params (dict): A dictionary of parameters to use in the filter query. Must include\n",
    "                       'field_list' and 'field_type' keys, which represent the list of field items (i.e., list of MGI model identifiers)\n",
    "                        to fetch and the type of the field (i.e., model_id) to filter on, respectively.\n",
    "        filename (str): The name of the file to write the results to. Defaults to 'iteration_solr_request'.\n",
    "        format (str): The format of the output file. Can be 'csv' or 'json'. Defaults to 'json'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate format\n",
    "    if format not in ['json','csv']:\n",
    "        raise ValueError(\"Invalid format. Please use 'json' or 'csv'\")\n",
    "    \n",
    "    # Base URL\n",
    "    base_url = \"https://www.ebi.ac.uk/mi/impc/solr/\"\n",
    "    solr_url = base_url + core + \"/select\"\n",
    "\n",
    "    # Extract entities_list and entity_type from params\n",
    "    field_list = params.pop(\"field_list\")\n",
    "    field_type = params.pop(\"field_type\")\n",
    "\n",
    "    # Construct the filter query with grouped model IDs\n",
    "    fq = \"{}:({})\".format(\n",
    "        field_type, \" OR \".join(['\"{}\"'.format(id) for id in field_list])\n",
    "    )\n",
    "\n",
    "    # Show users the field and field values they passed to the function\n",
    "    print(\"Queried field:\",fq)\n",
    "    # Set internal params the users should not change\n",
    "    params[\"fq\"] = fq\n",
    "    params[\"wt\"] = 'json'\n",
    "    params[\"start\"]=0 # Start at the first result\n",
    "    params[\"rows\"]=5000 # Fetch results in chunks of 5000\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Fetch results using a generator function\n",
    "        results_generator = entity_iterator(solr_url, params)\n",
    "    except Exception as e:\n",
    "        raise Exception(\"An error occurred while downloading the data: \" + str(e))\n",
    "\n",
    "    # Append extension to the filename\n",
    "    filename = f\"{filename}.{format}\"\n",
    "\n",
    "    try:\n",
    "        # Open the file in write mode\n",
    "        with open(filename, \"w\", newline=\"\") as f:\n",
    "            if format == 'csv':\n",
    "                writer = None\n",
    "                for item in results_generator:\n",
    "                    # Initialize the CSV writer with the keys of the first item as the field names\n",
    "                    if writer is None:\n",
    "                        writer = csv.DictWriter(f, fieldnames=item.keys())\n",
    "                        writer.writeheader()\n",
    "                    # Write the item to the CSV file\n",
    "                    writer.writerow(item)\n",
    "                    # Write to json without loading to memory.\n",
    "            elif format == 'json':\n",
    "                f.write('[')\n",
    "                for i, item in enumerate(results_generator):\n",
    "                    if i != 0:\n",
    "                        f.write(',')\n",
    "                    json.dump(item, f)\n",
    "                f.write(']')\n",
    "    except Exception as e:\n",
    "        raise Exception(\"An error occurred while writing the file: \" + str(e))\n",
    "\n",
    "    print(f\"File {filename} was created.\")\n",
    "\n",
    "def parse_metadata(metadata_list):\n",
    "    metadata_dict = {}\n",
    "    for item in metadata_list:\n",
    "        key, value = item.split(' = ')\n",
    "        metadata_dict[key] = value\n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003b6da3-d2ce-4070-9f5d-1d97eca0da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request data for ECG\n",
    "field_list = 'experiment_id,specimen_id,observation_id,biological_sample_group,pipeline_stable_id,procedure_stable_id,phenotyping_center,production_center,external_sample_id,strain_name,sex,zygosity,date_of_birth,date_of_experiment,age_in_weeks,parameter_stable_id,parameter_name,data_point,unit,observation_type,metadata_group,weight,weight_date,weight_days_old,weight_parameter_stable_id,metadata,life_stage_name,gene_symbol'\n",
    "df = batch_request(\n",
    "    core='experiment',\n",
    "    params={\n",
    "        'q': 'procedure_name:\"Electrocardiogram (ECG)\"',\n",
    "        'fl': field_list\n",
    "    },\n",
    "    batch_size=50000\n",
    ")\n",
    "df.to_json(\"ECG_raw.json\", orient=\"records\")\n",
    "\n",
    "# Request data for HWT\n",
    "field_list = 'experiment_id,specimen_id,observation_id,biological_sample_group,pipeline_stable_id,procedure_stable_id,phenotyping_center,production_center,external_sample_id,strain_name,sex,zygosity,date_of_birth,date_of_experiment,age_in_weeks,parameter_stable_id,parameter_name,data_point,unit,observation_type,metadata_group,weight,weight_date,weight_days_old,weight_parameter_stable_id,metadata,life_stage_name,gene_symbol'\n",
    "df = batch_request(\n",
    "    core='experiment',\n",
    "    params={\n",
    "        'q': 'procedure_name:\"Heart Weight\"',\n",
    "        'fl': field_list\n",
    "    },\n",
    "    batch_size=50000\n",
    ")\n",
    "df.to_json(\"HWT_raw.json\", orient=\"records\")\n",
    "\n",
    "# Request data for ECH\n",
    "field_list = 'experiment_id,specimen_id,observation_id,biological_sample_group,pipeline_stable_id,procedure_stable_id,phenotyping_center,production_center,external_sample_id,strain_name,sex,zygosity,date_of_birth,date_of_experiment,age_in_weeks,parameter_stable_id,parameter_name,data_point,unit,observation_type,metadata_group,weight,weight_date,weight_days_old,weight_parameter_stable_id,metadata,life_stage_name,gene_symbol'\n",
    "df = batch_request(\n",
    "    core='experiment',\n",
    "    params={\n",
    "        'q': 'procedure_name:\"Echo\"',\n",
    "        'fl': field_list\n",
    "    },\n",
    "    batch_size=10000\n",
    ")\n",
    "df.to_json(\"ECH_raw.json\", orient=\"records\")\n",
    "\n",
    "# Request data for OWT\n",
    "field_list = 'experiment_id,specimen_id,observation_id,biological_sample_group,pipeline_stable_id,procedure_stable_id,phenotyping_center,production_center,external_sample_id,strain_name,sex,zygosity,date_of_birth,date_of_experiment,age_in_weeks,parameter_stable_id,parameter_name,data_point,unit,observation_type,metadata_group,weight,weight_date,weight_days_old,weight_parameter_stable_id,metadata,life_stage_name,gene_symbol'\n",
    "df = batch_request(\n",
    "    core='experiment',\n",
    "    params={\n",
    "        'q': 'procedure_name:\"Organ Weight\"',\n",
    "        'fl': field_list\n",
    "    },\n",
    "    batch_size=50000\n",
    ")\n",
    "df.to_json(\"OWT_raw.json\", orient=\"records\")\n",
    "\n",
    "# Request data for body weight (BWT)\n",
    "field_list = 'experiment_id,specimen_id,observation_id,biological_sample_group,pipeline_stable_id,procedure_stable_id,phenotyping_center,production_center,external_sample_id,strain_name,sex,zygosity,date_of_birth,date_of_experiment,age_in_weeks,parameter_stable_id,parameter_name,data_point,observation_type,metadata_group,metadata,life_stage_name,gene_symbol'\n",
    "df = batch_request(\n",
    "    core='experiment',\n",
    "    params={\n",
    "        'q': 'parameter_stable_id:*_CAL_001_001 OR parameter_stable_id:*_DXA_001_001 OR parameter_stable_id:*_BWT_001_001',\n",
    "        'fl': field_list\n",
    "    },\n",
    "    batch_size=50000\n",
    ")\n",
    "df.to_json(\"BWT_raw.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ce34b-6ef8-4f55-85a5-086566536d5a",
   "metadata": {},
   "source": [
    "# 1. Download Electrocardiogram (ECG) data\n",
    "We use `procedure_name` field to filter out Electrocardiogram data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d1b541-f8e6-44c1-9038-5ec843a073a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecg = pd.read_json('ECG_raw.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce282401-23e0-4fc0-836f-99a4e2a79c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename data_point -> value\n",
    "df_ecg = df_ecg.rename(columns={'data_point': 'value'})\n",
    "\n",
    "# Apply the function to the metadata column\n",
    "metadata_dicts = df_ecg['metadata'].apply(parse_metadata)\n",
    "\n",
    "# Convert list of dictionaries to a DataFrame\n",
    "metadata_df = pd.DataFrame(metadata_dicts.tolist())\n",
    "\n",
    "# Merge the new DataFrame with the original one\n",
    "result_df = pd.concat([df_ecg.drop(columns=['metadata']), metadata_df], axis=1)\n",
    "\n",
    "# Get parameter_unit list of dictionaries\n",
    "procID_list1 = [str(i) for i in range(1415, 1433)]\n",
    "procID_list2 = [str(i) for i in range(647, 654)]\n",
    "procID_list3 = ['108','126','932', '1134', '1156', '1000126']\n",
    "procID_list = procID_list1 + procID_list2 + procID_list3\n",
    "\n",
    "parameter_list = []\n",
    "for procID in procID_list:\n",
    "    response = requests.get('https://api.mousephenotype.org/impress/parameter/belongingtoprocedure/full/' + procID)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        parameter_unit = [{'parameterKey': d['parameterKey'], 'unit': d['unit']} for d in data]\n",
    "        parameter_list.extend(parameter_unit)\n",
    "    \n",
    "# Get dictionary of ID and units\n",
    "response = requests.get('https://api.mousephenotype.org/impress/unit/list')\n",
    "if response.status_code == 200:\n",
    "# Parse the JSON response\n",
    "   unit_dict = response.json()\n",
    "\n",
    "# Replace 'unit' values in parameter_unit list with values from unit_dict\n",
    "for item in parameter_list:\n",
    "    if str(item['unit']) in unit_dict:\n",
    "        item['unit'] = unit_dict[str(item['unit'])]\n",
    "\n",
    "# Convert parameter_unit list of dicts to a dictionary for easier lookup\n",
    "parameter_unit_dict = {item['parameterKey']: item['unit'] for item in parameter_list}\n",
    "\n",
    "# Add 'unit' column based on parameter_stable_id\n",
    "result_df['unit'] = result_df['parameter_stable_id'].map(parameter_unit_dict)\n",
    "\n",
    "# Specify list of columns in the final dataframe\n",
    "ecg_csv = \"experiment_id\tspecimen_id\tobservation_id\tbiological_sample_group\tpipeline_stable_id\tprocedure_stable_id\tphenotyping_center\tproduction_center\texternal_sample_id\tstrain_name\tsex\tzygosity\tdate_of_birth\tdate_of_experiment\tage_in_weeks\tparameter_stable_id\tparameter_name\tvalue\tunit\tobservation_type\tmetadata_group\tweight\tweight_date\tweight_days_old\tweight_parameter_stable_id\tAnalysis Software\tAnesthetic\tEquipment Manufacturer\tEquipment Model\tgene_symbol\tlife_stage_name\"\n",
    "ecg_columns = ecg_csv.split(\"\t\")\n",
    "result_df_slice = result_df.loc[:, ecg_columns]\n",
    "\n",
    "# Split the DataFrame\n",
    "ea_ecg_df = result_df_slice[result_df_slice['life_stage_name'] == 'Early adult'].drop(columns=['life_stage_name'])\n",
    "la_ecg_df = result_df_slice[result_df_slice['life_stage_name'] == 'Late adult'].drop(columns=['life_stage_name'])\n",
    "\n",
    "ea_ecg_df.to_csv(\"output/EA_ECG.csv\", index=False)\n",
    "la_ecg_df.to_csv(\"output/LA_ECG.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f4a3e-4b45-43fa-bdba-fcf1258f794c",
   "metadata": {},
   "source": [
    "# 2. Download Heart Weight (HWT) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7dddbe1-2c37-4a75-9eab-111788d15f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hwt = pd.read_json('HWT_raw.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94463cff-4948-43be-967e-7537c316edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename data_point -> value\n",
    "df_hwt = df_hwt.rename(columns={'data_point': 'value'})\n",
    "\n",
    "# Apply the function to the metadata column\n",
    "metadata_dicts = df_hwt['metadata'].apply(parse_metadata)\n",
    "\n",
    "# Convert list of dictionaries to a DataFrame\n",
    "metadata_df = pd.DataFrame(metadata_dicts.tolist())\n",
    "\n",
    "# Merge the new DataFrame with the original one\n",
    "result_df = pd.concat([df_hwt.drop(columns=['metadata']), metadata_df], axis=1)\n",
    "\n",
    "# Get parameter_unit list of dictionaries\n",
    "# procID was used for api from HWT page: https://www.mousephenotype.org/impress/ProcedureInfo?action=list&procID=601&pipeID=7\n",
    "# response = requests.get('https://api.mousephenotype.org/impress/parameter/belongingtoprocedure/full/601')\n",
    "procID_list1 = [str(i) for i in range(600, 607)]\n",
    "procID_list2 = ['986', '1038','1060','1072', '1091', '1108', '1141', '1163', '1292', '1332', '1333', '100141']\n",
    "procID_list3 = ['100']\n",
    "procID_list = procID_list1 + procID_list2 + procID_list3\n",
    "\n",
    "parameter_list = []\n",
    "for procID in procID_list:\n",
    "    response = requests.get('https://api.mousephenotype.org/impress/parameter/belongingtoprocedure/full/' + procID)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        parameter_unit = [{'parameterKey': d['parameterKey'], 'unit': d['unit']} for d in data]\n",
    "        parameter_list.extend(parameter_unit)\n",
    "    \n",
    "# Get dictionary of ID and units\n",
    "response = requests.get('https://api.mousephenotype.org/impress/unit/list')\n",
    "if response.status_code == 200:\n",
    "# Parse the JSON response\n",
    "   unit_dict = response.json()\n",
    "\n",
    "# Replace 'unit' values in parameter_unit list with values from unit_dict\n",
    "for item in parameter_list:\n",
    "    if str(item['unit']) in unit_dict:\n",
    "        item['unit'] = unit_dict[str(item['unit'])]\n",
    "\n",
    "# Convert parameter_unit list of dicts to a dictionary for easier lookup\n",
    "parameter_unit_dict = {item['parameterKey']: item['unit'] for item in parameter_list}\n",
    "\n",
    "# Add 'unit' column based on parameter_stable_id\n",
    "result_df['unit'] = result_df['parameter_stable_id'].map(parameter_unit_dict)\n",
    "\n",
    "# Specify list of columns in the final dataframe\n",
    "hwt_csv = \"experiment_id\tspecimen_id\tobservation_id\tbiological_sample_group\tpipeline_stable_id\tprocedure_stable_id\tphenotyping_center\tproduction_center\texternal_sample_id\tstrain_name\tsex\tzygosity\tdate_of_birth\tdate_of_experiment\tage_in_weeks\tparameter_stable_id\tparameter_name\tvalue\tunit\tobservation_type\tmetadata_group\tweight\tweight_date\tweight_days_old\tweight_parameter_stable_id\tEquipment manufacturer\tEquipment model\tgene_symbol\tlife_stage_name\"\n",
    "hwt_columns = hwt_csv.split(\"\t\")\n",
    "result_df_slice = result_df.loc[:, hwt_columns]\n",
    "\n",
    "# Split the DataFrame\n",
    "ea_hwt_df = result_df_slice[result_df_slice['life_stage_name'] == 'Early adult'].drop(columns=['life_stage_name'])\n",
    "la_hwt_df = result_df_slice[result_df_slice['life_stage_name'] == 'Late adult'].drop(columns=['life_stage_name'])\n",
    "\n",
    "ea_hwt_df.to_csv(\"output/EA_HWT.csv\", index=False)\n",
    "la_hwt_df.to_csv(\"output/LA_HWT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f3b2f-1d87-4801-84cc-542c75229328",
   "metadata": {},
   "source": [
    "# 3. Download Echo (ECH) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "803d8807-c56e-404d-9f7e-2ba65fcdc47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ech = pd.read_json('ECH_raw.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c69ea9e-3714-4907-9bd2-55eb7f366408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impressTime(row):\n",
    "    pipeline_stable_id = row['pipeline_stable_id']\n",
    "    procedure_stable_id = row['procedure_stable_id']\n",
    "    \n",
    "    # Get scheduleId for procedure_stable_id\n",
    "    response = requests.get('https://api.mousephenotype.org/impress/procedure/bykey/' + procedure_stable_id)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "    schedule_list = []\n",
    "    for dictionary in data:\n",
    "        schedule_list.append(dictionary['scheduleId'])\n",
    "    \n",
    "    # Based on scheduleId and pipeline_stable_id get time\n",
    "    response = requests.get('https://api.mousephenotype.org/impress/schedule/belongingtopipeline/full/' + pipeline_stable_id)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        schedule = response.json()\n",
    "        \n",
    "    for dict_pipeline in schedule:\n",
    "        for schedule_id in schedule_list:\n",
    "            if dict_pipeline['scheduleId'] == schedule_id:\n",
    "                return dict_pipeline['time']\n",
    "\n",
    "# Rename data_point -> value\n",
    "df_ech = df_ech.rename(columns={'data_point': 'value', 'weight_date': 'body_weight_doe'})\n",
    "\n",
    "# Apply the function to the metadata column\n",
    "metadata_dicts = df_ech['metadata'].apply(parse_metadata)\n",
    "\n",
    "# Convert list of dictionaries to a DataFrame\n",
    "metadata_df = pd.DataFrame(metadata_dicts.tolist())\n",
    "\n",
    "# Merge the new DataFrame with the original one\n",
    "result_df = pd.concat([df_ech.drop(columns=['metadata']), metadata_df], axis=1)\n",
    "\n",
    "# Get parameter_unit list of dictionaries\n",
    "# procID was used for api from ECH page: https://www.mousephenotype.org/impress/ProcedureInfo?action=list&procID=654&pipeID=7\n",
    "response = requests.get('https://api.mousephenotype.org/impress/parameter/belongingtoprocedure/full/654')\n",
    "procID_list1 = [str(i) for i in range(654, 659)]\n",
    "procID_list2 = ['109', '450', '1046', '1065', '1082', '1115']\n",
    "procID_list3 = ['1256']\n",
    "procID_list = procID_list1 + procID_list2 + procID_list3\n",
    "\n",
    "parameter_list = []\n",
    "for procID in procID_list:\n",
    "    response = requests.get('https://api.mousephenotype.org/impress/parameter/belongingtoprocedure/full/' + procID)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        parameter_unit = [{'parameterKey': d['parameterKey'], 'unit': d['unit']} for d in data]\n",
    "        parameter_list.extend(parameter_unit)\n",
    "    \n",
    "# Get dictionary of ID and units\n",
    "response = requests.get('https://api.mousephenotype.org/impress/unit/list')\n",
    "if response.status_code == 200:\n",
    "# Parse the JSON response\n",
    "   unit_dict = response.json()\n",
    "\n",
    "# Replace 'unit' values in parameter_unit list with values from unit_dict\n",
    "for item in parameter_list:\n",
    "    if str(item['unit']) in unit_dict:\n",
    "        item['unit'] = unit_dict[str(item['unit'])]\n",
    "\n",
    "# Convert parameter_unit list of dicts to a dictionary for easier lookup\n",
    "parameter_unit_dict = {item['parameterKey']: item['unit'] for item in parameter_list}\n",
    "\n",
    "# Add 'unit' column based on parameter_stable_id\n",
    "result_df['unit'] = result_df['parameter_stable_id'].map(parameter_unit_dict)\n",
    "\n",
    "# Add 'impressTimepoint'\n",
    "pipeline_procedure = result_df.groupby(['pipeline_stable_id', 'procedure_stable_id']).size().reset_index(name='count')    \n",
    "pipeline_procedure['impress_time'] = pipeline_procedure.apply(get_impressTime, axis=1)\n",
    "pipeline_procedure['impress_time'] = pipeline_procedure['impress_time'].astype(int)\n",
    "\n",
    "# Merge the dataframes on 'pipeline_stable_id' and 'procedure_stable_id'\n",
    "merged_df = pd.merge(result_df, pipeline_procedure[['pipeline_stable_id', 'procedure_stable_id', 'impress_time']],\n",
    "                     on=['pipeline_stable_id', 'procedure_stable_id'], how='left')\n",
    "\n",
    "# Rename the 'impress_time' column to 'impressTimepoint'\n",
    "merged_df = merged_df.rename(columns={'impress_time': 'impressTimepoint'})\n",
    "\n",
    "# Add column with age in weeks for body_weight\n",
    "merged_df['body_weight_age_in_weeks'] = merged_df['weight_days_old'].apply(lambda x: x // 7 if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Calculate age_in_weeks-timepoint\n",
    "merged_df['(age_in_weeks-timepoint)'] = abs(merged_df['body_weight_age_in_weeks'] - merged_df['impressTimepoint'])\n",
    "\n",
    "# Specify list of columns in the final dataframe\n",
    "ech_csv = \"experiment_id\tspecimen_id\tobservation_id\tbiological_sample_group\tpipeline_stable_id\tprocedure_stable_id\tphenotyping_center\tproduction_center\texternal_sample_id\tstrain_name\tsex\tzygosity\tdate_of_birth\tdate_of_experiment\tage_in_weeks\tparameter_stable_id\tparameter_name\tvalue\tunit\tobservation_type\tmetadata_group\tweight\tweight_days_old\tweight_parameter_stable_id\tAnesthetic\tEquipment Manufacturer\tEquipment Model\timpressTimepoint\t(age_in_weeks-timepoint)\tgene_symbol\tlife_stage_name\"\n",
    "ech_columns = ech_csv.split(\"\t\")\n",
    "result_df_slice = merged_df.loc[:, ech_columns]\n",
    "\n",
    "# Split the DataFrame\n",
    "ea_ech_df = result_df_slice[result_df_slice['life_stage_name'] == 'Early adult'].drop(columns=['life_stage_name'])\n",
    "la_ech_df = result_df_slice[result_df_slice['life_stage_name'] == 'Late adult'].drop(columns=['life_stage_name'])\n",
    "\n",
    "ea_ech_df.to_csv(\"output/EA_ECH.csv\", index=False)\n",
    "la_ech_df.to_csv(\"output/LA_ECH.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0be53-ab34-4049-933a-cc0b06de5651",
   "metadata": {},
   "source": [
    "# 4. Download Organ Weight (OWT) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c021d9aa-fb77-4f06-af0f-36bf604efc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owt = pd.read_json('OWT_raw.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c863ca3c-fdb5-432e-90ab-2237fe332ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename data_point -> value\n",
    "df_owt = df_owt.rename(columns={'data_point': 'value'})\n",
    "\n",
    "# Apply the function to the metadata column\n",
    "metadata_dicts = df_owt['metadata'].apply(parse_metadata)\n",
    "\n",
    "# Convert list of dictionaries to a DataFrame\n",
    "metadata_df = pd.DataFrame(metadata_dicts.tolist())\n",
    "\n",
    "# Merge the new DataFrame with the original one\n",
    "result_df = pd.concat([df_owt.drop(columns=['metadata']), metadata_df], axis=1)\n",
    "\n",
    "# Get parameter_unit list of dictionaries\n",
    "procID_list1 = [str(i) for i in range(939, 944)]\n",
    "procID_list2 = ['106', '247', '275','1059', '1125', '1229', '1263', '1291', '1366', '1412']\n",
    "procID_list3 = ['1000247', '1000939', '1000940', '1000941', '1000942', '1000943', '1001125', '1000106']\n",
    "procID_list4 = [str(i) for i in range(600, 607)]\n",
    "procID_list5 = ['986', '1038','1060','1072', '1091', '1108', '1141', '1163', '1292', '1332', '1333', '100141']\n",
    "procID_list6 = ['100']\n",
    "procID_list = procID_list1 + procID_list2 + procID_list3 + procID_list4 + procID_list5 + procID_list6\n",
    "\n",
    "parameter_list = []\n",
    "for procID in procID_list:\n",
    "    response = requests.get('https://api.mousephenotype.org/impress/parameter/belongingtoprocedure/full/' + procID)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        parameter_unit = [{'parameterKey': d['parameterKey'], 'unit': d['unit']} for d in data]\n",
    "        parameter_list.extend(parameter_unit)\n",
    "    \n",
    "# Get dictionary of ID and units\n",
    "response = requests.get('https://api.mousephenotype.org/impress/unit/list')\n",
    "if response.status_code == 200:\n",
    "# Parse the JSON response\n",
    "   unit_dict = response.json()\n",
    "\n",
    "# Replace 'unit' values in parameter_unit list with values from unit_dict\n",
    "for item in parameter_list:\n",
    "    if str(item['unit']) in unit_dict:\n",
    "        item['unit'] = unit_dict[str(item['unit'])]\n",
    "\n",
    "# Convert parameter_unit list of dicts to a dictionary for easier lookup\n",
    "parameter_unit_dict = {item['parameterKey']: item['unit'] for item in parameter_list}\n",
    "\n",
    "# Add 'unit' column based on parameter_stable_id\n",
    "result_df['unit'] = result_df['parameter_stable_id'].map(parameter_unit_dict)\n",
    "\n",
    "# Specify list of columns in the final dataframe\n",
    "owt_csv = \"experiment_id\tspecimen_id\tobservation_id\tbiological_sample_group\tpipeline_stable_id\tprocedure_stable_id\tphenotyping_center\tproduction_center\texternal_sample_id\tstrain_name\tsex\tzygosity\tdate_of_birth\tdate_of_experiment\tage_in_weeks\tparameter_stable_id\tparameter_name\tvalue\tunit\tobservation_type\tmetadata_group\tweight\tweight_date\tweight_days_old\tweight_parameter_stable_id\tEquipment manufacturer\tEquipment model\tgene_symbol\tlife_stage_name\"\n",
    "owt_columns = owt_csv.split(\"\t\")\n",
    "result_df_slice = result_df.loc[:, owt_columns]\n",
    "\n",
    "# Split the DataFrame\n",
    "ea_owt_df = result_df_slice[result_df_slice['life_stage_name'] == 'Early adult'].drop(columns=['life_stage_name'])\n",
    "la_owt_df = result_df_slice[result_df_slice['life_stage_name'] == 'Late adult'].drop(columns=['life_stage_name'])\n",
    "\n",
    "ea_owt_df.to_csv(\"output/EA_OWT.csv\", index=False)\n",
    "la_owt_df.to_csv(\"output/LA_OWT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b0f1b-f77d-4a88-9a97-cae25e18b09e",
   "metadata": {},
   "source": [
    "# 5. Download Body Weight (BWT) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca0f365-1337-4ef6-941b-d0381ddaca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bwt = pd.read_json('BWT_raw.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93380bd3-2dab-4ed4-a931-87d04177c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename data_point -> value\n",
    "df_bwt = df_bwt.rename(columns={'data_point': 'value'})\n",
    "\n",
    "# Apply the function to the metadata column\n",
    "metadata_dicts = df_bwt['metadata'].apply(parse_metadata)\n",
    "\n",
    "# Convert list of dictionaries to a DataFrame\n",
    "metadata_df = pd.DataFrame(metadata_dicts.tolist())\n",
    "\n",
    "# Merge the new DataFrame with the original one\n",
    "result_df = pd.concat([df_bwt.drop(columns=['metadata']), metadata_df], axis=1)\n",
    "\n",
    "# Get parameter_unit list of dictionaries\n",
    "procID_list1 = [str(i) for i in range(623, 632)]\n",
    "procID_list2 = ['90', '103', '135', '137', '329', '331', '336', '346', '388', '369', '402', '423', '431', '432', '454', '462', '467', '470', '471', '477', '1076', '1186', '1094', '1102', '1120', '1137','1151', '1158', '1159', '1165', '1166', '1259', '1384' ]\n",
    "procID_list3 = ['1314', '1330', '992', '1000135', '1000942', '1000103', '1000329', '1000331', '1000336', '1000338', '1000431', '1000432', '1000454']\n",
    "procID_list4 = [str(i) for i in range(548, 556)]\n",
    "procID_list5 = ['86', '121', '153', '240', '977', '1045', '1057', '1081', '1110', '1191', '1254']\n",
    "procID_list6 = [str(i) for i in range(524, 530)]\n",
    "procID_list7 = [str(i) for i in range(696, 703)]\n",
    "procID_list8 = [str(i) for i in range(852, 860)]\n",
    "procID_list = procID_list1 + procID_list2 + procID_list3 + procID_list4 + procID_list5 + procID_list6 + procID_list7 + procID_list8\n",
    "\n",
    "parameter_list = []\n",
    "for procID in procID_list:\n",
    "    response = requests.get('https://api.mousephenotype.org/impress/parameter/belongingtoprocedure/full/' + procID)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        parameter_unit = [{'parameterKey': d['parameterKey'], 'unit': d['unit']} for d in data]\n",
    "        parameter_list.extend(parameter_unit)\n",
    "    \n",
    "# Get dictionary of ID and units\n",
    "response = requests.get('https://api.mousephenotype.org/impress/unit/list')\n",
    "if response.status_code == 200:\n",
    "# Parse the JSON response\n",
    "   unit_dict = response.json()\n",
    "\n",
    "# Replace 'unit' values in parameter_unit list with values from unit_dict\n",
    "for item in parameter_list:\n",
    "    if str(item['unit']) in unit_dict:\n",
    "        item['unit'] = unit_dict[str(item['unit'])]\n",
    "\n",
    "# Convert parameter_unit list of dicts to a dictionary for easier lookup\n",
    "parameter_unit_dict = {item['parameterKey']: item['unit'] for item in parameter_list}\n",
    "\n",
    "# Add 'unit' column based on parameter_stable_id\n",
    "result_df['unit'] = result_df['parameter_stable_id'].map(parameter_unit_dict)\n",
    "\n",
    "# Specify list of columns in the final dataframe\n",
    "bwt_csv = \"experiment_id\tspecimen_id\tobservation_id\tbiological_sample_group\tpipeline_stable_id\tprocedure_stable_id\tphenotyping_center\tproduction_center\texternal_sample_id\tstrain_name\tsex\tzygosity\tdate_of_birth\tdate_of_experiment\tage_in_weeks\tparameter_stable_id\tparameter_name\tvalue\tunit\tobservation_type\tmetadata_group\tEquipment manufacturer\tEquipment model\tgene_symbol\tlife_stage_name\"\n",
    "bwt_columns = bwt_csv.split(\"\t\")\n",
    "result_df_slice = result_df.loc[:, bwt_columns]\n",
    "\n",
    "# Split the DataFrame\n",
    "ea_bwt_df = result_df_slice[result_df_slice['life_stage_name'] == 'Early adult'].drop(columns=['life_stage_name'])\n",
    "la_bwt_df = result_df_slice[result_df_slice['life_stage_name'] == 'Late adult'].drop(columns=['life_stage_name'])\n",
    "\n",
    "ea_bwt_df.to_csv(\"output/EA_BWT.csv\", index=False)\n",
    "la_bwt_df.to_csv(\"output/LA_BWT.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
